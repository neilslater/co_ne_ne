RuNeNe TODOs:

    Refactoring:
        move conversions C enum <-> Ruby Symbol
        objective_function_spec DRY
        use vtables and function lookups to build a training routine from components

    RuNeNe::Trainer::BPLayer
      - start a training batch
      - backprop algorithm
      - apply batch changes to weights

    Objective functions
        Flag to allow optimisation of dE_dz with mlogloss + softmax, only when targets meet simplicity requirements

    RuNeNe::DataSet
      - Support for no output, maybe generating output for predictions
      - Allow support for splitting/re-organising data
      - Rename C and Ruby source files for new name

    Completely re-architect layer/network
      layer_ff consists solely of num_inputs, num_outputs, weights, transfer function * DONE
      trainer adds data structures for tracking weight deltas etc
      network adds architecture (initially just a stack of layers)

    RuNeNe::Trainer::BPNetwork

    Perform batch feed-forward runs

    Perform batch backprop runs

    Gradient-checking tests

    mini-batch training

    Tests using MNIST digits

    Easy save/load methods for all major objects

    transfer issues below to tickets in Github (how to give priorities and dependencies?)

----- v0.0.1 release ?

    Add Adadelta and.or Adagrad options, see http://imgur.com/a/Hqolp

    Parametric transfer functions?

    Metric functions

    Optionally take writable output array (for performance)

    improved vectorisation

    Bulk predictions

    Training set normalisation

    Training set split (into CV, Test etc)

    Dropout layer

    max norm regularisation

    l2 regularisation?

    rmsprop

    dynamic learning rate?

    auto-categorisation and category expansion

    training data auto-shuffle

    Residual gradients for RELU (& others?)

    Use SIMD for weight updates

    Holdout sets for generalisation

    Layer.clear_input (?)

    auto-encoders

    Split/Branch layers

    Maxout layer

    Import other NN trained models (cxxnet, caffe)







RuNeNe DONE:

    Training data shuffled inputs/outputs

    Training data Marshal

    Linear neurons

    Lock first layer in network, to prevent adding earlier layers

    Factor training cycle et al to struct_mlp_network

    Clone a network

    Fill starting NArrays with 0.0

    Init network from first layer

    Init network from array of layers (in Ruby)

    Marshalling, for layers and network

    Learning rate and momentum for network

    YARD docs

    Don't return ms_error on normal training

    Add training data class

    Fisher-Yates shuffle

    srand accept arrays

    Layer init weights

    Separate out cost/loss functions

    Log loss cost

    Softmax layer

    Combine cost de_da with da_dz to go direct to de_dz for most/all output deltas . . .
        Optimised combos for de_dz
        *  linear & mse
        *  sigmoid & logloss
        *  softmax & mlogloss
        *  relu & mse

        Non-optimised combos:
        *  everything else (block difficult or impossible combinations, such as logloss for linear?)