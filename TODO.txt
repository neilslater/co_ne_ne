RuNeNe TODOs:

    RuNeNe::Learn::MBGD
        allow global settings that apply to every layer
        run complete minibatch on NNModel, using DataSet

    RuNeNe::NeuralNetwork (?)
        Some container object for Network + Learn::MBGD that trains it

--

    RuNeNe::DataSet
      - Support for no output
      - Allow support for splitting/re-organising data
      - Rename C and Ruby source files for new name
      - Support for test and CV sets

    RuNeNe::Metrics
        - using Objective functions
        - Accuracy
        - AUROC
        - F1 score

    Bulk predictions

    Tests using MNIST digits

    Easy save/load methods for all major objects

    Documentation review

    Transfer issues below to tickets in Github (how to give priorities and dependencies?)

----- v0.0.1 release ?

    R_Engine
      - High-level "opcode" structs that contain direct references and structures to run a full
        netork + training quickly.
      - Built on-demand to perform batch learning and metrics/assessment tasks
      - allow for multiple layer types
      - whilst still efficient C struct calls
      - drop array of VALUEs in favour of array of mixed struct types?
      - want "build, then run" network methods that work with multiple layers

    Refactoring:
        move all array-walking/maths functions into same place for ease of later optimisations
        use vtables and function lookups to build a training routine from components
        more SIMD in backprop and gradient calculations
        are there larger SIMD vector sizes available on later Intel chips? https://en.wikipedia.org/wiki/Advanced_Vector_Extensions
        Windows compatibiliy
        rubinius compatibility (due to GC moving memory blocks)?
        Optimise mlogloss/softmax objective de_dz: Flag to allow optimisation when targets meet simplicity requirements
        Optimise away additions for softmax mid-layer support (stop them affecting other mid-layer calcs)
        Improve type errors to reference type of param received

    Add Adadelta and.or Adagrad options, see http://imgur.com/a/Hqolp
    - Later versions of RMSProp + Momentum
    - ESGD - http://arxiv.org/pdf/1502.04390.pdf ?
    - SMORMS3 ? - http://sifter.org/~simon/journal/20150420.html

    Parametric transfer functions?

    Metric functions

    Optionally take writable output array (for performance)

    improved vectorisation

    Training set normalisation

    Training set split (into CV, Test etc)

    Dropout layer

    dynamic learning rate?

    auto-categorisation and category expansion

    Residual gradients for RELU (& others?)

    Holdout sets for generalisation

    Auto-encoders

    Split/Branch layers

    Maxout layer

    Import other NN trained models (cxxnet, caffe)
