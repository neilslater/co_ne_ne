CoNeNe TODOs:

    srand accept arrays

    training data auto-shuffle

    Bulk training

    Bulk error calculations

    Use SIMD for weight updates

    Holdout sets for generalisation

    Layer.clear_input (?)


CoNeNe DONE:

    Lock first layer in network, to prevent adding earlier layers

    Factor training cycle et al to struct_mlp_network

    Clone a network

    Fill starting NArrays with 0.0

    Init network from first layer

    Init network from array of layers (in Ruby)

    Marshalling, for layers and network

    Learning rate and momentum for network

    YARD docs

    Don't return ms_error on normal training

    Add training data class

    Fisher-Yates shuffle
