RuNeNe TODOs:

    RuNeNe::Learn::MBGD::Layer
      - Max norm regularisation
      - L2 regularisation

    RuNeNe::Network

    RuNeNe::Learn::MBGD

    RuNeNe::DataSet
      - Support for no output, maybe generating output for predictions
      - Allow support for splitting/re-organising data
      - Rename C and Ruby source files for new name
      - Support for test and CV sets

    Mini-batch training

    Completely re-architect layer/network
      layer_ff consists solely of num_inputs, num_outputs, weights, transfer function * DONE
      trainer adds data structures for tracking weight deltas etc
      network adds architecture (initially just a stack of layers)

    Perform batch feed-forward runs

    Perform batch backprop runs

    Bulk predictions

    Tests using MNIST digits

    Easy save/load methods for all major objects

    Transfer issues below to tickets in Github (how to give priorities and dependencies?)


----- v0.0.1 release ?

    Refactoring:
        move all array-walking/maths functions into same place for ease of later optimisations
        use vtables and function lookups to build a training routine from components
        more SIMD in backprop and gradient calculations
        are there larger SIMD vector sizes available on later Intel chips? https://en.wikipedia.org/wiki/Advanced_Vector_Extensions
        Windows compatibiliy
        rubinius compatibility (due to GC moving memory blocks)?
        Optimise mlogloss/softmax objective de_dz: Flag to allow optimisation when targets meet simplicity requirements
        Optimise away additions for softmax mid-layer support (stop them affecting other mid-layer calcs)

    Add Adadelta and.or Adagrad options, see http://imgur.com/a/Hqolp
    - Later versions of RMSProp + Momentum
    - ESGD - http://arxiv.org/pdf/1502.04390.pdf ?
    - SMORMS3 ? - http://sifter.org/~simon/journal/20150420.html

    Parametric transfer functions?

    Metric functions

    Optionally take writable output array (for performance)

    improved vectorisation

    Training set normalisation

    Training set split (into CV, Test etc)

    Dropout layer

    dynamic learning rate?

    auto-categorisation and category expansion

    Residual gradients for RELU (& others?)

    Holdout sets for generalisation

    Auto-encoders

    Split/Branch layers

    Maxout layer

    Import other NN trained models (cxxnet, caffe)
