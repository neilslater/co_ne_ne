CoNeNe TODOs:

    Names:
        ruby_class_mlp_layer.c  ->  ruby_class_layer_feed_forward.c
        ruby_class_mlp_layer.h  ->
        VALUE mlp_layer_        ->  VALUE rv_layer_ff_
        ruby_class_mlp_network.c  ->  ruby_class_network.c
        ruby_class_mlp_network.h  ->  ruby_class_network.h


    Softmax layer

    Fix training data ( call it LabeledData ?)
        Support normalisation
        Support caterical data (getting expanded on-demand)

    Split training process between  network, training data and a "trainer" class

    Layer init weights

    Log loss cost

    improved vectorisation

    Tests using MNIST digits

    Bulk predictions

    Training set normalisation

    mini-batch training

    dropout

    max norm regularisation

    l2 regularisation?

    rmsprop

    dynamic learning rate?

    auto-categorisation and category expansion



    training data auto-shuffle

    Training data Marshal

    Bulk training

    Bulk error calculations

    Test that learning rate and momentum have an effect on training

    Residual gradients for RELU (& others?)

    ---- Start from here in 2014 ??? ---

    Use SIMD for weight updates

    Holdout sets for generalisation

    Layer.clear_input (?)

    auto-encoders


CoNeNe DONE:

    Linear neurons

    Lock first layer in network, to prevent adding earlier layers

    Factor training cycle et al to struct_mlp_network

    Clone a network

    Fill starting NArrays with 0.0

    Init network from first layer

    Init network from array of layers (in Ruby)

    Marshalling, for layers and network

    Learning rate and momentum for network

    YARD docs

    Don't return ms_error on normal training

    Add training data class

    Fisher-Yates shuffle

    srand accept arrays